{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyML0+Svwt5sFMttgiZmCt3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahirwarsuresh062-web/Ensemble-learning/blob/main/Ensemble_Learning_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART A: THEORY QUESTIONS (Easy Language)\n",
        "\n",
        "Q1. Can we use Bagging for regression problems?\n",
        "\n",
        "Ans. Yes, Bagging can be used for regression problems. In regression, Bagging trains multiple regression models on different bootstrap samples and averages their predictions to reduce error.\n",
        "\n",
        "Q2. Difference between multiple model training and single model training\n",
        "\n",
        "Ans. single model training uses only one model to make predictions.\n",
        "\n",
        "Multiple model training trains many models and combines their output to improve accuracy and stability.\n",
        "\n",
        "Q3. Feature randomness in Random Forest\n",
        "\n",
        "Ans. Random Forest selects a random subset of features for each split in a tree. This reduces correlation between trees and improves overall performance.\n",
        "\n",
        "Q4. What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "Ans. OOB score is an internal validation method where data not used in training a tree is used to test it. It helps estimate model accuracy without a separate test set.\n",
        "\n",
        "Q5. Feature importance in Random Forest\n",
        "\n",
        "Ans. Random Forest measures feature importance based on:\n",
        "\n",
        "Reduction in impurity\n",
        "\n",
        "Contribution of each feature to model accuracy\n",
        "\n",
        "Q6. Working principle of Bagging Classifier\n",
        "\n",
        "Ans. Create multiple bootstrap samples\n",
        "\n",
        "Train classifiers on each sample\n",
        "\n",
        "Combine predictions using majority voting\n",
        "\n",
        "Q7. Evaluation of Bagging Classifier\n",
        "\n",
        "Ans. Performance can be evaluated using:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision, Recall, F1-score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "Q8. How does Bagging Regressor work?\n",
        "\n",
        "Ans. Bagging Regressor trains multiple regressors and takes the average of predictions to reduce variance.\n",
        "\n",
        "Q9. Main advantage of ensemble techniques\n",
        "\n",
        "Ans. They increase accuracy and reduce overfitting compared to single models.\n",
        "\n",
        "Q10. Main challenge of ensemble methods\n",
        "\n",
        "Ans. High computational cost\n",
        "\n",
        "Less interpretability\n",
        "\n",
        "Q11. Key idea behind ensemble techniques\n",
        "\n",
        "Ans. Combining multiple weak models to create a strong model.\n",
        "\n",
        "Q12. What is a Random Forest Classifier?\n",
        "\n",
        "Ans. A Random Forest Classifier is an ensemble of decision trees trained using bagging and feature randomness.\n",
        "\n",
        "Q13. Types of ensemble techniques\n",
        "\n",
        "Ans. Bagging\n",
        "\n",
        "Boosting\n",
        "\n",
        "Stacking\n",
        "\n",
        "Q14. What is ensemble learning?\n",
        "\n",
        "Ans. Ensemble learning combines predictions from multiple models to improve performance.\n",
        "\n",
        "Q15. When should we avoid ensemble methods?\n",
        "\n",
        "Ans. When data is small\n",
        "\n",
        "When model interpretability is important\n",
        "\n",
        "Limited computing power\n",
        "\n",
        "Q16. How Bagging reduces overfitting\n",
        "\n",
        "Ans. By averaging predictions from multiple models trained on different samples.\n",
        "\n",
        "Q17. Why Random Forest is better than single Decision Tree\n",
        "\n",
        "Ans. It reduces overfitting and improves accuracy.\n",
        "\n",
        "Q18. Role of bootstrap sampling in Bagging\n",
        "\n",
        "Ans. Bootstrap sampling creates different datasets by random sampling with replacement.\n",
        "\n",
        "Q19. Real-world applications of ensemble techniques\n",
        "\n",
        "Ans. Fraud detection\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Stock market prediction\n",
        "\n",
        "Recommendation systems\n",
        "\n",
        "Q20. Difference between Bagging and Boosting\n",
        "\n",
        "Bagging\tBoosting\n",
        "\n",
        "Models trained independently\tModels trained sequentially\n",
        "\n",
        "Reduces variance\tReduces bias\n"
      ],
      "metadata": {
        "id": "CYtQ_qmmEVDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-B Practical Questions"
      ],
      "metadata": {
        "id": "nfVR3-UCFe1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans21. Bagging Classifier with Decision Tree\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmXt6RMYFpSH",
        "outputId": "81bc48dd-8009-45a9-e1de-1010fd0fef11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans22. Bagging Regressor with MSE\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "model = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50\n",
        ")\n",
        "model.fit(X, y)\n",
        "pred = model.predict(X)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwJwkNaHGEyu",
        "outputId": "94db9978-293d-4041-dfed-641a2295b4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 496.7668886877828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans23. Random Forest Classifier – Feature Importance\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(X, y)\n",
        "\n",
        "print(rf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hBDSVa7GFAl",
        "outputId": "76dcab98-0e85-4b0c-9711-a0a1a137e97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.04358491 0.01657074 0.0566336  0.05370734 0.00619668 0.00582527\n",
            " 0.03030234 0.10803607 0.0031623  0.00296843 0.0091137  0.00314255\n",
            " 0.00696574 0.05403568 0.00418359 0.00497882 0.00803527 0.00684839\n",
            " 0.00526496 0.00518077 0.14325934 0.01936686 0.12675686 0.11401296\n",
            " 0.01382592 0.01271619 0.02453337 0.09431863 0.00860227 0.00787042]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans24.Random Forest vs Decision Tree\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "dt = DecisionTreeRegressor()\n",
        "dt.fit(X, y)\n",
        "\n",
        "print(\"DT MSE:\", mean_squared_error(y, dt.predict(X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a45wNSKfGFNy",
        "outputId": "2e722799-f936-4dd2-ad8c-349d7553b6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DT MSE: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans25. OOB Score\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,\n",
        "    bootstrap=True\n",
        ")\n",
        "rf.fit(X, y)\n",
        "\n",
        "print(\"OOB Score:\", rf.oob_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFoWWw17G8QC",
        "outputId": "e9b995ef-8bc1-4e78-c528-8ba7aece9949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans26. Random Forest with GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'n_estimators':[50,100],\n",
        "    'max_depth':[5,10,None]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(), params)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(grid.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0jvmKWNG8cU",
        "outputId": "1aa2d9de-02de-456a-a07d-12871e527297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 10, 'n_estimators': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans27. StackingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True))\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "print(\"Stacking Accuracy:\", stack.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dqd-AZjG8pW",
        "outputId": "14c7b867-f504-461f-bcd5-4a3c35935622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Accuracy: 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans28. Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5K5vniAG85r",
        "outputId": "bc440971-54e1-4652-e6e6-3fe143dfaae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 3 2]\n",
            " [6 4 6]\n",
            " [4 2 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans29. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(), param_grid)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou_BGw62JDNt",
        "outputId": "e4f2f038-e25e-4762-e0d1-11cc41eb8024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 100}\n",
            "Best Accuracy: 0.9666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ana30.Train a Bagging Regressor with different numbers of estimators\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "for n in [10, 50, 100]:\n",
        "    model = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n\n",
        "    )\n",
        "    model.fit(X, y)\n",
        "    pred = model.predict(X)\n",
        "    print(f\"Estimators: {n}, MSE:\", mean_squared_error(y, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXemxNd_JAkD",
        "outputId": "102c91b6-b36e-49b2-84a9-d007cfea4ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimators: 10, MSE: 623.6270135746606\n",
            "Estimators: 50, MSE: 478.4904660633484\n",
            "Estimators: 100, MSE: 454.71061266968326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans31. Train a Random Forest Classifier and analyze misclassified samples\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "pred = rf.predict(X_test)\n",
        "\n",
        "misclassified = X_test[pred != y_test]\n",
        "print(\"Number of Misclassified Samples:\", len(misclassified))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt-3VcPoJA3f",
        "outputId": "9eb83fc1-6a2c-47ee-f662-4461d81a0def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Misclassified Samples: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans32. Compare Bagging Classifier with single Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt.score(X_test, y_test))\n",
        "print(\"Bagging Accuracy:\", bag.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ah2ey8uJBAx",
        "outputId": "146112ec-88a1-4923-c05b-92565876dcea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans33. Random Forest Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Ensure X_train, y_train, X_test, y_test are from the Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier and get predictions\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "pred = rf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBRAklw9JBEU",
        "outputId": "e15bb5d7-8402-484d-85c7-7fda36c61bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans34. Stacking Classifier (DT + SVM + Logistic Regression)\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True))\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "print(\"Stacking Accuracy:\", stack.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KbUHyuOJBHi",
        "outputId": "551cb3bc-6a43-49e2-ff55-44ea9a227482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Accuracy: 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans35. Random Forest – Top 5 Important Features\n",
        "import numpy as np\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "print(\"Top 5 Features:\")\n",
        "for i in indices[:5]:\n",
        "    print(importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jfaLSGJJBK0",
        "outputId": "95569b7a-ccd5-4f74-eb1e-8bc444d32a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Features:\n",
            "0.12328635140149158\n",
            "0.12024825985108482\n",
            "0.11542114456349123\n",
            "0.11376376884389261\n",
            "0.11284888922284506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans36. Bagging Classifier – Precision, Recall, F1-score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Eb4k5o3JBN_",
        "outputId": "b14c603c-4302-49a9-bdcc-233ca5af4fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.2407407407407407\n",
            "Recall: 0.2261904761904762\n",
            "F1 Score: 0.21850877192982457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans37.Effect of max_depth on Random Forest Accuracy\n",
        "for depth in [3, 5, 10, None]:\n",
        "    rf = RandomForestClassifier(max_depth=depth)\n",
        "    rf.fit(X_train, y_train)\n",
        "    print(f\"Max Depth {depth} Accuracy:\", rf.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8s8cjAzLXb_",
        "outputId": "2cced4a4-9ecd-4ed1-ed97-6199d1ce5824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Depth 3 Accuracy: 1.0\n",
            "Max Depth 5 Accuracy: 1.0\n",
            "Max Depth 10 Accuracy: 1.0\n",
            "Max Depth None Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans38. Bagging Regressor with Decision Tree & KNN\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "models = [\n",
        "    DecisionTreeRegressor(),\n",
        "    KNeighborsRegressor()\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    bag = BaggingRegressor(estimator=model, n_estimators=50)\n",
        "    bag.fit(X, y)\n",
        "    pred = bag.predict(X)\n",
        "    print(model.__class__.__name__, \"MSE:\", mean_squared_error(y, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmkYJ3cFLf9d",
        "outputId": "62004a68-7d42-408d-f336-1e9e4621269d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeRegressor MSE: 536.7758895927602\n",
            "KNeighborsRegressor MSE: 2242.987424760181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans39. Random Forest – ROC AUC Score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "prob = rf.predict_proba(X_test)\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, prob, multi_class='ovr'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvpcW795LXqk",
        "outputId": "b9fade55-d13d-4b9f-8d5e-f488e5a06669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans40. Bagging Classifier – Cross Validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(bag, X, y, cv=5)\n",
        "print(\"Cross Validation Scores:\", scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eUxbruELXuK",
        "outputId": "0e415ec0-a559-4420-887b-6089614cb997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation Scores: [1.      0.95296 0.      0.7     0.     ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans41. Precision-Recall Curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Ensure X_train, y_train, X_test, y_test are from the Iris dataset and rf is fitted\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Calculate probabilities\n",
        "prob = rf.predict_proba(X_test)\n",
        "\n",
        "# For multi-class, precision_recall_curve works for one class vs rest.\n",
        "# Let's consider class 1. Binarize y_test for class 1.\n",
        "y_test_binary = (y_test == 1)\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test_binary, prob[:,1])\n",
        "print(\"Precision values:\", precision[:5])\n",
        "print(\"Recall values:\", recall[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mA4HXqOLXxt",
        "outputId": "ded16cd3-a64c-4b62-9541-4f2825e8a6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision values: [0.3        0.5        0.5625     0.6        0.64285714]\n",
            "Recall values: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans42. Stacking (Random Forest + Logistic Regression)\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier()),\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "print(\"Stacking Accuracy:\", stack.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1pX5hxaLX1h",
        "outputId": "4c9e925a-4bb4-43e8-9299-e42d3e42e415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans43. Bagging Regressor – Different Bootstrap Levels\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "# Ensure X and y are loaded for the BaggingRegressor example\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "for bootstrap in [True, False]:\n",
        "    bag = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=50,\n",
        "        bootstrap=bootstrap\n",
        "    )\n",
        "    bag.fit(X, y)\n",
        "    pred = bag.predict(X)\n",
        "    print(\"Bootstrap:\", bootstrap, \"MSE:\", mean_squared_error(y, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C4GiItYLX5n",
        "outputId": "9419663e-0910-4f18-e45e-971b10f3510a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap: True MSE: 485.6286606334842\n",
            "Bootstrap: False MSE: 0.0\n"
          ]
        }
      ]
    }
  ]
}